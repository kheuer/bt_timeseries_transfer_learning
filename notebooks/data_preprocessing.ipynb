{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48862e4a-63c6-4007-887a-d85c37cc193b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dffe10a8-9b87-4c93-8977-b9ced7de367d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "from io import StringIO\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from darts.datasets import AirPassengersDataset, ETTh1Dataset, ETTh2Dataset, ETTm1Dataset, ETTm2Dataset, ElectricityDataset, EnergyDataset, ExchangeRateDataset, ILINetDataset, TaylorDataset, TrafficDataset, USGasolineDataset, UberTLCDataset, WeatherDataset\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5180a1-8877-4704-acf1-f5a8ed505b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "801c91ed-5e4b-441d-89b7-e7fe267aee46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/koos/Documents/timeseries_transfer_learning'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.getcwd()[-9:] == \"notebooks\":\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59cf9b52-f7af-445a-bc58-84d707754f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_nan_at_ends(df):\n",
    "    first_idx = df.first_valid_index()\n",
    "    last_idx = df.last_valid_index()\n",
    "    return df.loc[first_idx:last_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3afb0c1-1773-4cec-8acb-e1cc97db9fe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3027d24-beaa-4250-9af5-087509978759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_directory = 'data/raw_download/stocks'\n",
    "output_directory = 'data/processed/stocks'\n",
    "test_directory = \"data/processed/test\"\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    print(\"created output_directory\")\n",
    "    \n",
    "if not os.path.exists(test_directory):\n",
    "    os.makedirs(test_directory)\n",
    "    print(\"created test_directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666414ba-097a-4010-9c79-5cf4e3405d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_movement(df):\n",
    "    return bool((df.max() - df.min()).sum())\n",
    "\n",
    "NAN_THRESHOLD_STOCKS = 0.005\n",
    "def test_nan_acceptable(df):\n",
    "    return ((df.isna().sum().sum()/4)/len(df)) <= NAN_THRESHOLD_STOCKS\n",
    "    \n",
    "def test_high_low_logical(df):\n",
    "    return all(((df[\"High\"].round(4) >= df[\"Open\"].round(4)).all(),\n",
    "                (df[\"High\"].round(4) >= df[\"Low\"].round(4)).all(),\n",
    "                (df[\"High\"].round(4) >= df[\"Close\"].round(4)).all(),\n",
    "                (df[\"Low\"].round(4) <= df[\"Open\"].round(4)).all(),\n",
    "                (df[\"Low\"].round(4) <= df[\"Close\"].round(4)).all()))\n",
    "\n",
    "def test_iliquid(df):\n",
    "    return ((df == df.shift(1)) &\n",
    "            (df == df.shift(2)) &\n",
    "            (df == df.shift(3)) &\n",
    "            (df == df.shift(4)) &\n",
    "            (df == df.shift(5)) &\n",
    "            (df == df.shift(6)) &\n",
    "            (df == df.shift(7)) &\n",
    "            (df == df.shift(8)) &\n",
    "            (df == df.shift(9)) &\n",
    "            (df == df.shift(10)) &\n",
    "            (df == df.shift(11)) &\n",
    "            (df == df.shift(12)) &\n",
    "            (df == df.shift(13)) &\n",
    "            (df == df.shift(14)) &\n",
    "            (df == df.shift(15)) &\n",
    "            (df == df.shift(16)) &\n",
    "            (df == df.shift(17)) &\n",
    "            (df == df.shift(18)) &\n",
    "            (df == df.shift(19)) &\n",
    "            (df == df.shift(20)) &\n",
    "            (df == df.shift(21)) &\n",
    "            (df == df.shift(22)) &\n",
    "            (df == df.shift(23)) &\n",
    "            (df == df.shift(24)) &\n",
    "            (df == df.shift(25)) &\n",
    "            (df == df.shift(26)) &\n",
    "            (df == df.shift(27)) &\n",
    "            (df == df.shift(28)) &\n",
    "            (df == df.shift(29)) &\n",
    "            (df == df.shift(30)) &\n",
    "            (df == df.shift(31)) &\n",
    "            (df == df.shift(32)) &\n",
    "            (df == df.shift(33)) &\n",
    "            (df == df.shift(34)) &\n",
    "            (df == df.shift(35))\n",
    "            ).any().any()\n",
    "\n",
    "def test_df(df):\n",
    "    \"\"\"\n",
    "    return True if df quality is acceptable\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(df) < 200:# dataframe is too short\n",
    "        return False, \"length\"\n",
    "    \n",
    "    if (df == 0).any().any():# dataframe contains a zero, price cant be zero\n",
    "        return False, \"zero\"\n",
    "    \n",
    "    if (df.pct_change()[1:] >= 2.5).any().any():# movement is unrealistic, drops or increases too much\n",
    "        return False, \"unrealistic_movement\"\n",
    "    \n",
    "    if not test_high_low_logical(df):# high is lower than other or low is higher than other\n",
    "        return False, \"high_low\"\n",
    "    \n",
    "    if test_iliquid(df):\n",
    "        return False, \"illiquid\"\n",
    "    \n",
    "    \n",
    "    return True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02213aff-35f9-4521-8371-85030a660705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_files = glob.glob(os.path.join(input_directory, '*.csv'))\n",
    "np.random.shuffle(csv_files)\n",
    "len(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3282655-9d01-4bd7-91ac-4fdd0a5b6932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors = {\"length\": 0,\n",
    "          \"zero\": 0,\n",
    "          \"no_movement\": 0,\n",
    "          \"nan\": 0,\n",
    "          \"high_low\": 0,\n",
    "          \"unrealistic_movement\": 0,\n",
    "          \"illiquid\": 0\n",
    "         }\n",
    "\n",
    "for i in tqdm(range(len(csv_files))):\n",
    "    file = csv_files[i]\n",
    "    ticker = file[25:-4]\n",
    "    try:\n",
    "        df = pd.read_csv(file)[[\"Open\", \"High\", \"Low\", \"Close\"]]\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"parseerror for ticker\", ticker)\n",
    "        continue\n",
    "    \n",
    "    if not test_nan_acceptable(df):\n",
    "        errors[\"nan\"] += 1\n",
    "        continue\n",
    "    \n",
    "    df = df.ffill()\n",
    "    \n",
    "    test, error = test_df(df)\n",
    "    if not test:\n",
    "        errors[error] += 1\n",
    "        continue\n",
    "\n",
    "    for col in [\"Close\"]:\n",
    "        df_ = df[[col]]\n",
    "\n",
    "\n",
    "        df_.to_parquet(f\"data/processed/stocks/{ticker}-{col}.parquet\", engine='pyarrow')\n",
    "\n",
    "print(\"Processing complete.\", errors)\n",
    "print(\"n_files:\", len(os.listdir(\"data/processed/stocks\")), len(os.listdir(\"data/processed/stocks\")) == 35873+6330)\n",
    "print(\"errors:\", sum(errors.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30490da4-fa9a-4f0a-b435-6581aed662fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get a list of files in the stocks folder\n",
    "files_in_stocks = os.listdir(output_directory)\n",
    "\n",
    "# Choose a random sample of 20 files from the stocks folder\n",
    "random_files = np.random.choice(files_in_stocks, int(0.15*len(files_in_stocks)), replace=False)\n",
    "\n",
    "# Move the selected files to the test folder\n",
    "for file_name in random_files:\n",
    "    file_path = os.path.join(output_directory, file_name)\n",
    "    destination_path = os.path.join(test_directory, file_name)\n",
    "    shutil.move(file_path, destination_path)\n",
    "    #print(file_path, destination_path)\n",
    "print(f\"Moved files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942de3c-eb65-499e-a461-e3fdc0e313fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22724352-e17e-4daa-bc84-5c8380949f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_anualized_std_weather(df, periods_in_year=252):\n",
    "    daily_std = np.std(np.array(df))\n",
    "    annualized_std = daily_std * np.sqrt(periods_in_year)\n",
    "    return annualized_std\n",
    "\n",
    "def add_missing_dates(df):\n",
    "    #df['ELEMENT'] = pd.to_datetime(df['DATE'])\n",
    "    #df.set_index('ELEMENT', inplace=True)\n",
    "    df.sort_index()\n",
    "\n",
    "    date_range = pd.date_range(start=df.index.min(), end=df.index.max())\n",
    "    df = df.reindex(date_range)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce2418-ce5b-4a64-ae1f-0be11c148823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Metadata specs #\n",
    "\n",
    "metadata_col_specs = [\n",
    "    (0,  12),\n",
    "    (12, 21),\n",
    "    (21, 31),\n",
    "    (31, 38),\n",
    "    (38, 41),\n",
    "    (41, 72),\n",
    "    (72, 76),\n",
    "    (76, 80),\n",
    "    (80, 86)\n",
    "]\n",
    "\n",
    "metadata_names = [\n",
    "    \"ID\",\n",
    "    \"LATITUDE\",\n",
    "    \"LONGITUDE\",\n",
    "    \"ELEVATION\",\n",
    "    \"STATE\",\n",
    "    \"NAME\",\n",
    "    \"GSN FLAG\",\n",
    "    \"HCN/CRN FLAG\",\n",
    "    \"WMO ID\"]\n",
    "\n",
    "metadata_dtype = {\n",
    "    \"ID\": str,\n",
    "    \"STATE\": str,\n",
    "    \"NAME\": str,\n",
    "    \"GSN FLAG\": str,\n",
    "    \"HCN/CRN FLAG\": str,\n",
    "    \"WMO ID\": str\n",
    "    }\n",
    "\n",
    "\n",
    "# Data specs #\n",
    "\n",
    "data_header_names = [\n",
    "    \"ID\",\n",
    "    \"YEAR\",\n",
    "    \"MONTH\",\n",
    "    \"ELEMENT\"]\n",
    "\n",
    "data_header_col_specs = [\n",
    "    (0,  11),\n",
    "    (11, 15),\n",
    "    (15, 17),\n",
    "    (17, 21)]\n",
    "\n",
    "data_header_dtypes = {\n",
    "    \"ID\": str,\n",
    "    \"YEAR\": int,\n",
    "    \"MONTH\": int,\n",
    "    \"ELEMENT\": str}\n",
    "\n",
    "data_col_names = [[\n",
    "    \"VALUE\" + str(i + 1),\n",
    "    \"MFLAG\" + str(i + 1),\n",
    "    \"QFLAG\" + str(i + 1),\n",
    "    \"SFLAG\" + str(i + 1)]\n",
    "    for i in range(31)]\n",
    "# Join sub-lists\n",
    "data_col_names = sum(data_col_names, [])\n",
    "\n",
    "data_replacement_col_names = [[\n",
    "    (\"VALUE\", i + 1),\n",
    "    (\"MFLAG\", i + 1),\n",
    "    (\"QFLAG\", i + 1),\n",
    "    (\"SFLAG\", i + 1)]\n",
    "    for i in range(31)]\n",
    "# Join sub-lists\n",
    "data_replacement_col_names = sum(data_replacement_col_names, [])\n",
    "data_replacement_col_names = pd.MultiIndex.from_tuples(\n",
    "    data_replacement_col_names,\n",
    "    names=['VAR_TYPE', 'DAY'])\n",
    "\n",
    "data_col_specs = [[\n",
    "    (21 + i * 8, 26 + i * 8),\n",
    "    (26 + i * 8, 27 + i * 8),\n",
    "    (27 + i * 8, 28 + i * 8),\n",
    "    (28 + i * 8, 29 + i * 8)]\n",
    "    for i in range(31)]\n",
    "data_col_specs = sum(data_col_specs, [])\n",
    "\n",
    "data_col_dtypes = [{\n",
    "    \"VALUE\" + str(i + 1): int,\n",
    "    \"MFLAG\" + str(i + 1): str,\n",
    "    \"QFLAG\" + str(i + 1): str,\n",
    "    \"SFLAG\" + str(i + 1): str}\n",
    "    for i in range(31)]\n",
    "data_header_dtypes.update({k: v for d in data_col_dtypes for k, v in d.items()})\n",
    "\n",
    "\n",
    "# Reading functions #\n",
    "\n",
    "def read_station_metadata(filename=\"ghcnd-stations.txt\"):\n",
    "    \"\"\"Reads in station metadata\n",
    "\n",
    "    :filename: ghcnd station metadata file.\n",
    "    :returns: station metadata as a pandas Dataframe\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.read_fwf(filename, metadata_col_specs, names=metadata_names,\n",
    "                     index_col='ID', dtype=metadata_dtype)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_ghcn_data_file(filename=None,\n",
    "                        variables=None, include_flags=False,\n",
    "                        dropna='all'):\n",
    "    \"\"\"Reads in all data from a GHCN .dly data file\n",
    "\n",
    "    :param filename: path to file\n",
    "    :param variables: list of variables to include in output dataframe\n",
    "        e.g. ['TMAX', 'TMIN', 'PRCP']\n",
    "    :param include_flags: Whether to include data quality flags in the final output\n",
    "    :returns: Pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_fwf(\n",
    "        filename,\n",
    "        colspecs=data_header_col_specs + data_col_specs,\n",
    "        names=data_header_names + data_col_names,\n",
    "        index_col=data_header_names,\n",
    "        dtype=data_header_dtypes\n",
    "        )\n",
    "\n",
    "\n",
    "    df.columns = data_replacement_col_names\n",
    "\n",
    "    df = df.stack(level='DAY').unstack(level='ELEMENT')\n",
    "\n",
    "    if dropna:\n",
    "        df.replace(-9999.0, np.nan, inplace=True)\n",
    "        df.dropna(how=dropna, inplace=True)\n",
    "\n",
    "    # replace the entire index with the date.\n",
    "    # This loses the station ID index column!\n",
    "    # This will usuall fail if dropna=False, since months with <31 days\n",
    "    # still have day=31 columns\n",
    "    df.index = pd.to_datetime(\n",
    "        df.index.get_level_values('YEAR') * 10000 +\n",
    "        df.index.get_level_values('MONTH') * 100 +\n",
    "        df.index.get_level_values('DAY'),\n",
    "        format='%Y%m%d')\n",
    "\n",
    "    return df[\"VALUE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06110ce-28ca-40d1-abe6-f512d9d54535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/noaa_stations.json', 'r') as file:\n",
    "    stations = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606bd10-67b7-4c50-96c3-ebf394d228f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_directory = \"data/raw_download/weather\"\n",
    "output_directory = 'data/processed/weather'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    print(\"created output directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c9981-d6c7-4d87-b20c-132cb9fa20a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NAN_THRESHOLD = 0.01\n",
    "WRONG_THRESHOLD = 0.03\n",
    "MIN_LENGTH = 200 # can be different for weather and stocks\n",
    "\n",
    "\n",
    "\n",
    "for file in tqdm(os.listdir(input_directory)):\n",
    "    station = file[:-4]\n",
    "    df = read_ghcn_data_file(f\"data/raw_download/weather/{file}\")\n",
    "    df = add_missing_dates(df)\n",
    "    best_len = 0\n",
    "    saved_df = False\n",
    "    \n",
    "    \n",
    "    for col in (\"TMIN\", \"TMAX\", \"TAVG\"):\n",
    "        if not col in df.columns:\n",
    "            # column not in data for this station\n",
    "            continue\n",
    "\n",
    "        df_ = df[[col]]\n",
    "        df_ = drop_nan_at_ends(df_)\n",
    "\n",
    "        if len(df_) < MIN_LENGTH:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        nan_percentage = float(df_.isna().sum().sum()/len(df_))\n",
    "        if nan_percentage > NAN_THRESHOLD:\n",
    "            continue\n",
    "        df_ = df_.interpolate()\n",
    "        \n",
    "        value_range = (df_.max()[0] - df_.min()[0])\n",
    "        rolling_average = df_[col].rolling(window=10).mean()\n",
    "        condition = abs(df_[col] - rolling_average) >= 0.2 * value_range\n",
    "        df_.loc[condition, col] = float(\"nan\") \n",
    "        \n",
    "        \n",
    "        wrong_data_percentage = float(df_.isna().sum().sum()/len(df_))\n",
    "        \n",
    "        if wrong_data_percentage > WRONG_THRESHOLD:\n",
    "            continue\n",
    "        elif not df[col].max() and not df[col].min():\n",
    "            # all entries are zero, e.g. snow in africa\n",
    "            continue\n",
    "        df_ = df_.interpolate()\n",
    "        \n",
    "\n",
    "        \n",
    "        if len(df_) > best_len:\n",
    "            best_len = len(df_)\n",
    "            col_save = col\n",
    "            df_save = df_\n",
    "            saved_df = True\n",
    "    \n",
    "\n",
    "    if saved_df:\n",
    "        output_file = f\"data/processed/weather/{station}-{col_save}.parquet\"\n",
    "        df_save.to_parquet(output_file, engine='pyarrow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b9e18-beb5-4ef1-96ec-ebb09a3c16a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# All Darts Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bce30e7-d3d7-4e35-8808-704fc4ee84c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_directory = 'data/processed/darts'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    print(\"created output directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660ca69e-273a-46fb-9e63-3adb3246c635",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n):\n",
    "    if isinstance(a, pd.DataFrame):\n",
    "        a = np.array(a)\n",
    "    \n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return pd.DataFrame(ret[n - 1:] / n)\n",
    "\n",
    "def save_timeseries(timeseries, name, moving_average_window=None):\n",
    "    if isinstance(timeseries, pd.DataFrame):\n",
    "        df = timeseries\n",
    "    elif isinstance(timeseries, pd.Series):\n",
    "        df = timeseries.to_frame()\n",
    "    else:\n",
    "        df = timeseries.pd_dataframe()\n",
    "    df = pd.DataFrame(np.array(df))\n",
    "\n",
    "    nan_percent = float(df.isna().sum().sum()/len(df))\n",
    "    if nan_percent > 0.01:\n",
    "        print(f\"didnt save {name} because of nan\")\n",
    "        return\n",
    "    df = df.interpolate()\n",
    "    df = drop_nan_at_ends(df)\n",
    "    if moving_average_window is not None:\n",
    "        df = moving_average(df, moving_average_window)\n",
    "    \n",
    "    df.to_parquet(output_directory + f\"/{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03299a3-dbe6-49aa-a73a-12cfc9c090e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = ETTh1Dataset().load()[\"OT\"]\n",
    "save_timeseries(data, \"ETTh1Dataset\", 20)\n",
    "print(ETTh1Dataset)\n",
    "data = ETTh2Dataset().load()[\"OT\"]\n",
    "save_timeseries(data, \"ETTh2Dataset\", 20)\n",
    "print(ETTh2Dataset)\n",
    "data = ETTm1Dataset().load()[\"OT\"]\n",
    "save_timeseries(data, \"ETTm1Dataset\")\n",
    "print(ETTm1Dataset)\n",
    "data = ETTm2Dataset().load()[\"OT\"]\n",
    "save_timeseries(data, \"ETTm2Dataset\")\n",
    "print(ETTm2Dataset)\n",
    "dataset = ElectricityDataset(multivariate=False).load()\n",
    "for col in dataset:\n",
    "    name = list(col.columns)[0]\n",
    "    save_timeseries(col, f\"ElectricityDataset_{name}\", 20)\n",
    "dataset = EnergyDataset().load().pd_dataframe()\n",
    "for col in [\"generation biomass\", \"generation fossil gas\", \"generation fossil oil\", \"generation other renewable\", \"generation waste\", \"total load forecast\", \"generation fossil hard coal\", \"generation hydro run-of-river and poundage\", \"generation other\", \"generation wind onshore\"]:\n",
    "    timeseries = dataset[col]\n",
    "    save_timeseries(timeseries, f\"EnergyDataset_{col}\")\n",
    "print(EnergyDataset)\n",
    "\n",
    "dataset = ExchangeRateDataset(multivariate=False).load()\n",
    "for i, timeseries in enumerate(dataset):\n",
    "    if i == 4:\n",
    "        continue\n",
    "    save_timeseries(timeseries, f\"ExchangeRateDataset_{i}\")\n",
    "print(ExchangeRateDataset)\n",
    "dataset = ILINetDataset(multivariate=False).load()\n",
    "for i, timeseries in enumerate(dataset):\n",
    "    if i in [3, 4, 6]:\n",
    "        continue\n",
    "    save_timeseries(timeseries, f\"ILINetDataset_{i}\", 20)\n",
    "print(ILINetDataset)\n",
    "dataset = TrafficDataset().load().pd_dataframe()\n",
    "for col in dataset.columns:\n",
    "    timeseries = dataset[col]\n",
    "    save_timeseries(timeseries, f\"TrafficDataset_{col}\", 40)\n",
    "print(TrafficDataset)\n",
    "dataset = USGasolineDataset().load().pd_dataframe()\n",
    "save_timeseries(dataset, \"USGasolineDataset\", 10)\n",
    "print(USGasolineDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only Exchange Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created output directory\n"
     ]
    }
   ],
   "source": [
    "output_directory = 'data/processed/fx'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    print(\"created output directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ExchangeRateDataset(multivariate=False).load()\n",
    "for i, timeseries in enumerate(dataset):\n",
    "    if i == 4:\n",
    "        continue\n",
    "    save_timeseries(timeseries, f\"ExchangeRateDataset_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6aae02-b3d2-4e4a-8af2-70125ab4d90f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Make Plots from Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93da498-cbed-4dd5-bbd5-8d880257c439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files_stock = glob.glob(os.path.join(\"data/processed/stocks/\", '*.parquet')) + glob.glob(os.path.join(\"data/processed/test/\", '*.parquet'))\n",
    "files_weather = glob.glob(os.path.join(\"data/processed/weather/\", '*.parquet'))\n",
    "files_darts = glob.glob(os.path.join(\"data/processed/darts/\", '*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd301f-6a60-4bf7-97ed-80441f816aaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show(files):\n",
    "    for i in range(10):\n",
    "        file = np.random.choice(files)\n",
    "        df = pd.read_parquet(file)[:1000]\n",
    "        \n",
    "        df = np.array(df)\n",
    "        \n",
    "        #plt.plot(df)\n",
    "        #plt.plot(moving_average(np.array(df), 10))\n",
    "        #plt.title(str(file).split(\"\\\\\")[-1][:-8])\n",
    "        #plt.title(file)\n",
    "        #plt.show()\n",
    "\n",
    "show(files_darts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f6187-1566-4632-98e2-ab5035d2be87",
   "metadata": {},
   "source": [
    "# Show Data Split by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725fe35e-a1a1-465d-be37-9bc26093cf66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_stock_dp, n_weather_dp, n_darts_dp = 0, 0, 0\n",
    "for file in tqdm(files_stock):\n",
    "    n_stock_dp += len(pd.read_parquet(file))\n",
    "print(f\"Stock timeseries: {len(files_stock)} stocks\\nMean length: {int(n_stock_dp/len(files_stock))} workdays\\nTotal datapoints: {round(n_stock_dp/1_000_000, 2)} million\\n\")\n",
    "\n",
    "for file in tqdm(files_weather):\n",
    "    n_weather_dp += len(pd.read_parquet(file))\n",
    "print(f\"Weather timeseries: {len(files_weather)} stations\\nMean length: {int(n_weather_dp/len(files_weather))} days\\nTotal datapoints: {round(n_weather_dp/1_000_000, 2)} million\\n\")\n",
    "\n",
    "for file in tqdm(files_darts):\n",
    "    n_darts_dp += len(pd.read_parquet(file))\n",
    "print(f\"Darts timeseries: {len(files_darts)} measurements\\nMean length: {int(n_darts_dp/len(files_darts))} observations\\nTotal datapoints: {round(n_darts_dp/1_000_000, 2)} million\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
