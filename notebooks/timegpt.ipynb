{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85273e08-05fe-4561-a7f9-dc79d06bac85",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bfcedaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import sys\n",
    "from nixtlats import TimeGPT\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b304c34f-c6cb-4cf9-b899-7a62c0bf5810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/koos/Documents/timeseries_transfer_learning'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.getcwd()[-9:] == \"notebooks\":\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d32486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metric import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3156ff5-f002-4a9b-986b-cffe281f672c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec5d058-d10a-4eb0-bc66-a9d1db66fc98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features_targets(df, features_length, targets_length):\n",
    "    if len(df) < features_length + targets_length:\n",
    "        raise ValueError(\"features and targets canÂ´t be longer than data\")\n",
    "    \n",
    "    min_start_targets = features_length\n",
    "    max_start_targets = len(df) - targets_length\n",
    "    targets_start = np.random.randint(min_start_targets, max_start_targets)\n",
    "    targets_end = targets_start + targets_length\n",
    "    features_start = targets_start - features_length\n",
    "    features, targets = df[features_start:targets_start], df[targets_start:targets_end]\n",
    "    assert len(targets) == targets_length\n",
    "    assert len(features) == features_length\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb4ad0-3069-4664-84bb-3eb74debbdc6",
   "metadata": {},
   "source": [
    "# Make calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c670122-7366-44b0-8bc7-28306eb7bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(f\"results/timegpt/timegpt_results_{WINDOW_SIZE}.json\", \"r\") as file:\n",
    "        results = json.load(file)\n",
    "except FileNotFoundError:\n",
    "    input(\"FILE NOUT FOUND, WRITING NEW ONE PRESS ENTER TO CONTINUE \")\n",
    "    results = {}\n",
    "\n",
    "done = list(results.keys())\n",
    "print(f\"{len(done)} calls were already done, skipping\")\n",
    "\n",
    "api_key = \"insert api key here\"\n",
    "timegpt = TimeGPT(token=api_key)\n",
    "assert timegpt.validate_token()\n",
    "\n",
    "skipped = 0\n",
    "files = os.listdir(\"data/processed/test\")\n",
    "for file in tqdm(files):\n",
    "    #print(\"calling data for\", file)\n",
    "    key = file.split(\"\\\\\")[-1][:-8]\n",
    "    if key in done:\n",
    "        continue\n",
    "    ticker = key.replace(\"-High\", \"\").replace(\"-Low\", \"\").replace(\"-Close\", \"\")\n",
    "    \n",
    "    dates = pd.read_csv(f\"data/raw_download/stocks/{ticker}.csv\")\n",
    "\n",
    "    df = pd.read_parquet(\"data/processed/test/\" + file)\n",
    "    col = df.columns[0]\n",
    "\n",
    "    assert len(df) == len(dates)\n",
    "    df[\"Date\"] = pd.to_datetime(dates[\"Date\"], utc=True)\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "    if len(df) <= WINDOW_SIZE + 14:\n",
    "        skipped += 1\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    features, targets = get_features_targets(df, WINDOW_SIZE, 14)\n",
    "    \n",
    "    data_for_timegpt = {}\n",
    "    for i, row in features.iterrows():\n",
    "        data_for_timegpt[i.strftime(\"%Y-%m-%d\")] = row.sum()\n",
    "    \n",
    "    \n",
    "    url = \"https://dashboard.nixtla.io/api/timegpt\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"timegpt-1\",\n",
    "        \"freq\": \"D\",\n",
    "        \"fh\": len(targets),\n",
    "        \"y\": data_for_timegpt,\n",
    "        \"clean_ex_first\": True,\n",
    "        \"finetune_steps\": 100\n",
    "    }\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers, timeout=180)\n",
    "        output = json.loads(response.text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"FAILED DECODE:\", response.text)\n",
    "        continue\n",
    "    except (ConnectionError, requests.ConnectionError) as e:\n",
    "        print(f\"ConnectionError: {e}\")\n",
    "        continue\n",
    "    except (TimeoutError, requests.ReadTimeout) as e:\n",
    "        print(f\"Timeout error: {e}\")\n",
    "    if output[\"message\"] != \"success\":\n",
    "        print(\"FAILED:\", output[\"message\"])\n",
    "        continue\n",
    "\n",
    "    output_df = pd.DataFrame(output[\"data\"])\n",
    "    output_df[\"timestamp\"] = pd.to_datetime(output_df[\"timestamp\"])\n",
    "    output_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    #plt.plot(list(np.array(features).flatten()) + list(targets.iloc[:, 0]))\n",
    "    #plt.plot([None for _ in range(len(features))] + list(output_df.loc[:,\"value\"]))\n",
    "    #plt.show()\n",
    "\n",
    "    results[key] = {\"features\": list(features.iloc[:, 0]),\n",
    "                   \"targets\": list(targets.iloc[:, 0]),\n",
    "                   \"output\": output}\n",
    "    \n",
    "    with open(f\"results/timegpt/timegpt_results_{WINDOW_SIZE}.json\", \"w\") as file:\n",
    "        json.dump(results, file)\n",
    "    time.sleep(1)\n",
    "    \n",
    "print(\"skipped\", skipped, \"files because of insufficient length\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2779f9ab-90bd-4c61-a2fb-0f13e1f1f721",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyse prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f673ed58-40ad-41f4-9d2a-31f54e3b7f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_list(lst):\n",
    "    lst = np.array(lst)\n",
    "    min_value = min(lst)\n",
    "    max_value = max(lst)\n",
    "    if min_value == max_value:\n",
    "        return lst, (min_value, max_value)\n",
    "    else:\n",
    "        return list(((lst - min_value) / (max_value - min_value))), (min_value, max_value)\n",
    "\n",
    "def reverse_normalize_list(lst, factors):\n",
    "    min_value, max_value = factors\n",
    "    return [(x * (max_value - min_value)) + min_value for x in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3781f732-4966-4a71-ab0c-40930e7529dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7574"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"results/timegpt/timegpt_results_{WINDOW_SIZE}.json\", \"r\") as file:\n",
    "    results = json.load(file)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ceb482b-7db0-4a2c-9f22-8aa5ff23d013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_prediction(features, targets, prediction):\n",
    "    plt.plot(features, c=\"black\", label=\"features\")\n",
    "    plt.plot([None for _ in list(features)]+ list(targets), c=\"grey\", label=\"targets\")\n",
    "    plt.plot([None for _ in list(features)]+ list(prediction), c=\"red\", label=\"prediction\")\n",
    "    plt.legend()\n",
    "    plt.title(key)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b368642d-9b45-43e1-b601-59a2fb6627f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_targets = []\n",
    "all_preds = []\n",
    "all_features = []\n",
    "all_factors = []\n",
    "\n",
    "for key in results:\n",
    "    features = np.array(results[key][\"features\"])\n",
    "    targets = np.array(results[key][\"targets\"])\n",
    "    response = results[key][\"output\"]\n",
    "    prediction = np.array(response[\"data\"][\"value\"])\n",
    "    \n",
    "    _, (min_factor, max_factor) = normalize_list(list(features))\n",
    "\n",
    "    features_norm = ((features - min_factor) / (max_factor - min_factor))\n",
    "    targets_norm = ((targets - min_factor) / (max_factor - min_factor))\n",
    "    prediction_norm = ((prediction - min_factor) / (max_factor - min_factor))\n",
    "    \n",
    "    #plot_prediction(features_norm, targets_norm, prediction_norm)\n",
    "    \n",
    "    all_targets.append(targets_norm)\n",
    "    all_preds.append(prediction_norm)\n",
    "    all_features.append(darts.TimeSeries.from_series(features_norm))\n",
    "    all_factors.append((min_factor, max_factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8beb00b7-cdd9-4025-8141-74f35ce84ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_of_timeseries_to_tensor(lst):\n",
    "    tensor_builder = []\n",
    "    for t in lst:\n",
    "        print\n",
    "        tensor_builder.append(np.array(t))\n",
    "    return torch.Tensor(np.array(tensor_builder))\n",
    "\n",
    "preds_tensor = list_of_timeseries_to_tensor(all_preds)\n",
    "targets_tensor = list_of_timeseries_to_tensor(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91fdbfd9-3554-46ef-a9ed-48d37f2e19d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINDOW_SIZE: 504\n",
      "Loss for L1Loss(): 0.061544425785541534\n",
      "Loss for MedianAbsoluteError(): 0.045115046203136444\n",
      "Loss for MSELoss(): 0.010171704925596714\n",
      "Loss for MedianSquaredError(): 0.002819785615429282\n",
      "Loss for HuberLoss(): 0.004435580223798752\n",
      "Loss for MeanLastValueError(): 0.08869222551584244\n",
      "Loss for MedianLastValueError(): 0.06078791618347168\n",
      "Loss for MeanTotalReturnError(): 0.04512231796979904\n",
      "Loss for MedianTotalReturnError(): 0.02182612195611\n",
      "Loss for GeometricMeanDailyReturnError(): 0.008478716491269063\n",
      "Loss for MeanFinalReturnError(): 0.06402437539038958\n",
      "Loss for MedianFinalReturnError(): 0.02703884319274051\n",
      "Market Outperformance Analysis (top 5.0%):\n",
      "Market Benchmark: 0.004320308811488409\n",
      "Portfolio Return: 0.008543210457652654\n",
      "Outperformance: 0.004222901646164246\n",
      "Loss for BackTestingProfitError(): 0.004222901646164246\n"
     ]
    }
   ],
   "source": [
    "print(\"WINDOW_SIZE:\", len(all_features[0]))\n",
    "\n",
    "loss_functions = all_metrices\n",
    "\n",
    "\n",
    "for i, fn in enumerate(loss_functions):\n",
    "    if hasattr(fn, \"pass_features_and_normal_factors\"):\n",
    "        loss = fn(preds_tensor, targets_tensor, all_features, all_factors)\n",
    "    else:\n",
    "        loss = fn(preds_tensor, targets_tensor)\n",
    "\n",
    "    print(f\"Loss for {loss_functions[i]}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e6027-7f8f-40a9-96bd-c0902b974b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
